---
title: "Linear representations in language models can change dramatically over a conversation"
publishDate: "2026-02-01"
category: "paper-reading"
subcategory: "AI Interpretability"
tags:
  [
    "ai-interpretability",
    "language-models",
    "activation-steering",
    "representation-learning",
    "paper-reading",
  ]
date: "2026-02-01"
author: "Hiep Tran"
featured: false
image: "/imgs/blogs/linear-representations-in-language-models-can-change-dramatically-over-a-conversation-20260201164730.png"
excerpt: "A deep dive into how language models' internal representations of concepts like factuality and ethics can flip dramatically during conversations, challenging assumptions behind interpretability methods and raising important questions for AI safety."
---

## Motivation

Recent research suggests that large language models contain linear internal representations that correspond to high level concepts such as factuality, ethics, or safety. These representations are often treated as stable signals that can be used to interpret, monitor, or even control model behavior.

At the same time, language models are highly adaptive during conversations. As a dialogue unfolds, models adjust their behavior based on context, which can change what they say, how confident they are, and even what they appear to believe.

This creates a tension. If models constantly adapt to context, can we really assume that internal representations of concepts like “factual” or “non factual” stay fixed over time. The motivation of this paper is to investigate whether these internal representations remain stable, or whether they change as a conversation progresses.

## Contribution

First, the paper shows that internal representations of concepts such as factuality can change significantly during a conversation. A statement that is represented as non factual at one point can later be represented as factual after a few dialogue turns, and vice versa.

Second, these changes are systematic rather than random. They appear consistently across many layers of the model and persist even when the representations are known to be robust to other types of prompting.

Third, the authors show that these effects do not require special or on policy conversations. Similar changes occur when replaying conversations generated by other models or using scripted dialogues, suggesting that this is a general property of contextual adaptation.

Finally, the paper highlights important implications for interpretability and model monitoring. If the meaning of internal representations can shift during a conversation, then checking whether a model is “in a factual state” at a single point in time does not guarantee reliable behavior later. This challenges existing interpretability methods that assume stable meanings of internal features and points to the need for new approaches that account for dynamic, context dependent representations.

## Background

Research on neural networks has long suggested that models learn internal representations that capture meaningful structure along specific dimensions. In language models, this idea became especially influential after word embeddings were shown to encode concepts in a linear way, such as semantic relationships and analogies. More recent work shows that large language models also contain linear representations for higher level concepts like factuality, honesty, or ethics.

Several theories explain why such linear representations emerge. One explanation is that they reflect patterns in the training data itself. For example, true statements tend to appear together with other true statements, and false statements tend to co occur with false ones. Combined with the biases of gradient based training, this can lead models to develop linear dimensions that track concepts like truth.

At the same time, there is growing concern about interpretability. Many studies show that interpretations based on internal representations can be unreliable, especially when models are used outside their original data distribution. In particular, linear representations may not faithfully reflect a model’s actual beliefs or decision process.

Recent work also shows that internal representations can change due to context. Through in context learning, models update their internal states based on the conversation so far. These updates can reflect changes in how the model represents latent concepts that are relevant to the dialogue. This suggests that representations are not static, but evolve over time as context accumulates.

Overall, this background motivates the need to study how internal representations change during conversations, and what this means for the reliability and faithfulness of interpretability methods that assume stable representations.

## Methods

**Models**. The main experiments are conducted on Gemma 3 language models, with a particular focus on the largest 27B instruction tuned model. Additional experiments on other model sizes and families are reported in the appendix to test generality.

**Conversations and prompts**. The authors use a diverse set of conversations, stories, and prompts. These are collected from prior work, manually written, generated by large language models, or created through direct interaction with the models. Some conversations are produced on policy, while others are scripted or edited.

A conversation is divided into turns, where each turn consists of a user message followed by a model response. Individual messages within a turn are referred to as plies. The initial prompt before any messages is treated as turn zero.

**Question answer datasets**. They first generate questions using a language model. Then they rewrite each question so that the opposite answer becomes true. This ensures the dataset is balanced and not biased toward Yes or No. Unclear questions are removed.

Two types of questions are used.

- Generic questions are context independent, such as basic science facts or model identity questions.
- Context relevant questions depend on the ongoing conversation and are expected to change as the conversation evolves. These are created separately for each story or dialogue.

This setup allows the authors to study how the model’s internal understanding changes with context.

**Evaluating models and extracting representations**. The model is evaluated on both possible answers to each question, regardless of which answer it would naturally generate. Representations are extracted while the model processes the “Yes” or “No” token at each layer of the network. This allows the authors to analyze how the model internally represents factual versus non factual answers, rather than only observing surface behavior.

**Identifying linear dimensions**. To identify linear representations of concepts like factuality, the authors train regularized logistic regression classifiers on the extracted representations from generic questions. These classifiers predict whether a question answer pair is factual.

![](/imgs/blogs/linear-representations-in-language-models-can-change-dramatically-over-a-conversation-20260201224331.png)

The best performing layer is selected using a held out validation set evaluated on empty prompts and opposite day prompts. This setup helps separate conceptual representations from prompt specific behavior.

**Margin score**. To summarize results, the authors define a margin score that measures how well the learned linear direction separates factual and non factual answers in representation space. A large positive margin indicates clear separation, while a negative margin indicates that the representations are flipped, meaning the model internally treats incorrect answers as more factual than correct ones.

![](/imgs/blogs/linear-representations-in-language-models-can-change-dramatically-over-a-conversation-20260201232035.png)

## Experiments

![](/imgs/blogs/linear-representations-in-language-models-can-change-dramatically-over-a-conversation-20260202141908.png)

The figure shows a simple but revealing experiment using an “opposite day” prompt.

At the start of the conversation, a learned linear dimension clearly separates factual and non factual answers. However, after the model is instructed to answer with the opposite of the truth, this separation quickly degrades and then reverses. Non factual answers become more aligned with the “factual” direction, and vice versa.

The same flipping effect is observed for an ethics dimension, showing that this behavior is not unique to factuality. This suggests that the identified dimensions may reflect the model’s current answering behavior rather than stable internal concepts.

Overall, the experiment demonstrates that linear representations can change dramatically over the course of a conversation, highlighting the risk of interpreting such dimensions as fixed or conceptually faithful.

![](/imgs/blogs/linear-representations-in-language-models-can-change-dramatically-over-a-conversation-20260202151633.png)

The experiment shows that factuality representations behave differently depending on the type of question.

For generic, context independent questions, the model’s factuality representation remains relatively stable throughout the conversation. The model continues to distinguish factual from non factual answers.

In contrast, for questions that are directly related to the ongoing conversation, the representation gradually flips. Answers that are actually factual become aligned with the non factual direction, and vice versa.

This indicates that while general knowledge remains stable, the model’s internal notion of what is “factual” can change dramatically for conversation specific content as the dialogue evolves.

![](/imgs/blogs/linear-representations-in-language-models-can-change-dramatically-over-a-conversation-20260202161541.png)

The above figure shows that the observed representation changes are not specific to off-policy settings.

First, the authors repeat the experiment using an on-policy conversation, where the user responses are adapted to the model’s outputs. They find qualitatively similar results: representations for context-relevant questions still flip over the course of the conversation, while generic factuality remains relatively stable. This shows that the effect is not caused by replaying scripted dialogues.

Next, the authors study a role-play debate where the model alternates between pro- and anti-consciousness positions. As the model switches roles, its factuality representations oscillate accordingly, flipping back and forth over time.

These results suggest that representation shifts reflect the model’s current role or stance in the conversation, rather than a permanent change in its underlying knowledge or beliefs.

![](/imgs/blogs/linear-representations-in-language-models-can-change-dramatically-over-a-conversation-20260202161556.png)

This experiment shows that model representations change much less when the model is only telling a story.

The authors compare role playing conversations with story based interactions to separate the effect of content from the effect of playing a role. In the story setting, the model is asked to generate a story on a topic, and factuality representations are evaluated only after the story is finished.

They find much weaker representational changes in this case. This suggests that strong representation shifts are mainly driven by conversations where the model is implicitly playing a role or taking a stance, rather than by unusual or fictional content alone.

![](/imgs/blogs/linear-representations-in-language-models-can-change-dramatically-over-a-conversation-20260203133427.png)

The authors compare different model sizes within the Gemma 3 family.

They find that larger models show stronger representational changes over the course of a conversation. The 12B model exhibits similar but weaker effects compared to the 27B model, while the 4B model shows little to no change.

This suggests that larger models are more context adaptable. While this makes them better at in context learning, it also makes them more susceptible to effects such as representational drift and jailbreak like behaviors.

![](/imgs/blogs/linear-representations-in-language-models-can-change-dramatically-over-a-conversation-20260203133657.png)

The authors replicate the “opposite day” experiment using the Qwen3 14B model and observe qualitatively similar representational changes.

However, Qwen3 14B does not produce reliably factual answers on more nuanced questions, even in an empty context. Because this baseline performance is insufficient, the authors cannot evaluate the model on their later, more detailed experiments.

![](/imgs/blogs/linear-representations-in-language-models-can-change-dramatically-over-a-conversation-20260203134710.png)

The authors evaluate the unsupervised Contrast Consistent Search (CCS) method (proposed in [Discovering Latent Knowledge in Language Models Without Supervision
](https://arxiv.org/abs/2212.03827)) for identifying representation dimensions related to concepts like factuality. They test CCS on the same datasets and model layers used in their main experiments, repeating the procedure many times and evaluating only cases where CCS performs well on held out generic factual questions in an empty context.

They find that CCS can often identify useful dimensions in empty prompts, reproducing prior results. However, once a long conversation is introduced, CCS performance frequently degrades, sometimes falling below chance even on generic factual questions. For conversation specific questions, CCS shows similar flipping behavior to that observed in the main experiments: it performs above chance in empty contexts but often fails in conversational settings.

Overall, these results show that representations extracted by CCS are not robust to distribution shifts caused by extended conversations. This reinforces the paper’s main conclusion that interpretability methods relying on fixed linear representations can break down when model representations change dynamically with context.

![](/imgs/blogs/linear-representations-in-language-models-can-change-dramatically-over-a-conversation-20260203134912.png)

This section studies whether causal interventions on internal representations behave consistently across different contexts.

The authors identify representation directions from the token _before_ the model produces an answer that predict whether the model will answer “Yes” or “No.” They then intervene along these directions to bias the model toward factual answers.

In an empty context, the intervention works as expected, pushing the model to answer as if questions are factual. However, after a long conversation about chakras, the same intervention has the opposite effect, biasing the model toward non factual answers.

This shows that representation changes over a conversation can cause interventions to behave differently from how they were designed. Even if an intervention works in some long contexts, it does not guarantee consistent behavior across all conversations.

## Discussion

The paper shows that linear representations of concepts like factuality and ethics are highly context sensitive. During a conversation, what a model represents as factual or ethical can flip to non factual or unethical, and vice versa. These changes happen quickly, across different model families, and are stronger in larger models.

Scientifically, the results suggest that representation changes are best understood as a form of role adaptation. Models restructure their internal representations to fit the role or stance they are playing in the conversation, rather than undergoing a stable change in underlying beliefs. This interpretation is supported by the fact that similar effects appear in both on policy and off policy settings.

The findings highlight a double edged nature of contextual adaptation. The same mechanisms that allow models to perform powerful in context learning can also lead to problematic behaviors in long or unusual conversations, such as drifting away from factuality or safety.

Importantly, the results raise serious challenges for interpretability and safety methods. Linear probes or features that appear reliable in short or empty contexts can become misleading in long conversations, undermining assumptions behind tools like factuality detectors or sparse autoencoders. This shows that interpretability methods must account for representation dynamics over context, not assume fixed meanings.

Looking forward, the authors argue that understanding how and why representations change during conversations is crucial for building more robust interpretability, safer model monitoring, and better control methods. They also acknowledge limitations, including limited conversational coverage and incomplete causal understanding, and emphasize the ethical relevance of this work for issues such as jailbreaks and delusional behavior in language models.

## References

1. [Linear representations in language models can change dramatically over a conversation](https://arxiv.org/pdf/2601.20834)
